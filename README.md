![alt text](image.png)

## LLM?

![alt text](LLM1.png)

### LLM Models

| Model Name     | #Parameters                                   | Release | Base Models | Open Source | Type            | #Tokens  | Training Dataset                                                                 |
| -------------- | --------------------------------------------- | ------- | ----------- | ----------- | --------------- | -------- | -------------------------------------------------------------------------------- |
| BERT           | 110M, 340M                                    | 2018    | -           | ✓           | Encoder-Only    | 137B     | BooksCorpus, English Wikipedia                                                   |
| RoBERTa        | 355M                                          | 2019    | -           | ✓           | Encoder-Only    | 2.2T     | BooksCorpus, English Wikipedia, CC-NEWS, STORIES, Reddit                         |
| ALBERT         | 12M, 18M, 60M, 235M                           | 2019    | -           | ✓           | Encoder-Only    | 137B     | BooksCorpus, English Wikipedia                                                   |
| DeBERTa        | -                                             | 2020    | -           | ✓           | Encoder-Only    | -        | BooksCorpus, English Wikipedia, STORIES, Reddit content                          |
| XLNet          | 110M, 340M                                    | 2019    | -           | ✓           | Encoder-Only    | 32.89B   | BooksCorpus, English Wikipedia, Giga5, Common Crawl, ClueWeb 2012-B              |
| GPT-1          | 120M                                          | 2018    | -           | ✓           | Decoder-Only    | 1.3B     | BooksCorpus                                                                      |
| GPT-2          | 1.5B                                          | 2019    | -           | ✓           | Decoder-Only    | 10B      | Reddit outbound                                                                  |
| T5 (Base)      | 223M                                          | 2019    | -           | ✓           | Encoder-Decoder | 156B     | Common Crawl                                                                     |
| MT5 (Base)     | 300M                                          | 2020    | -           | ✓           | Encoder-Decoder | -        | New Common Crawl-based dataset in 101 languages                                  |
| BART (Base)    | 139M                                          | 2019    | -           | ✓           | Encoder-Decoder | -        | Corrupting text                                                                  |
| GPT-3          | 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13B, 175B | 2020    | -           | ×           | Decoder-Only    | 300B     | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia                     |
| CODEX          | 12B                                           | 2021    | GPT         | ✓           | Decoder-Only    | -        | Public GitHub software repositories                                              |
| WebGPT         | 760M, 13B, 175B                               | 2021    | GPT-3       | ×           | Decoder-Only    | -        | ELI5                                                                             |
| GPT-4          | 1.76T                                         | 2023    | -           | ×           | Decoder-Only    | 13T      | -                                                                                |
| LLaMA1         | 7B, 13B, 33B, 65B                             | 2023    | -           | ✓           | Decoder-Only    | 1T, 1.4T | Online sources                                                                   |
| LLaMA2         | 7B, 13B, 34B, 70B                             | 2023    | -           | ✓           | Decoder-Only    | 2T       | Online sources                                                                   |
| LLaMA 3.1      | 7B, 13B, 65B                                  | 2024    | -           | ✓           | Decoder-Only    | 3T       | Web data, research papers, books, code repositories, and conversational datasets |
| Alpaca         | 7B                                            | 2023    | LLaMA1      | ✓           | Decoder-Only    | -        | GPT-3.5                                                                          |
| Vicuna-13B     | 13B                                           | 2023    | LLaMA1      | ✓           | Decoder-Only    | -        | GPT-3.5                                                                          |
| Koala          | 13B                                           | 2023    | LLaMA       | ✓           | Decoder-Only    | -        | Dialogue data                                                                    |
| Mistral-7B     | 7.3B                                          | 2023    | -           | ✓           | Decoder-Only    | -        | -                                                                                |
| Code Llama     | 34B                                           | 2023    | LLaMA2      | ✓           | Decoder-Only    | 500B     | Publicly available code                                                          |
| LongLLaMA      | 3B, 7B                                        | 2023    | OpenLLaMA   | ✓           | Decoder-Only    | 1T       | -                                                                                |
| PaLM           | 8B, 62B, 540B                                 | 2022    | -           | ×           | Decoder-Only    | 780B     | Web documents, books, Wikipedia, conversations, GitHub code                      |
| U-PaLM         | 8B, 62B, 540B                                 | 2022    | -           | ×           | Decoder-Only    | 1.3B     | Web documents, books, Wikipedia, conversations, GitHub code                      |
| PaLM-2         | 340B                                          | 2023    | -           | ✓           | Decoder-Only    | 3.6T     | Web documents, books, code, mathematics, conversational data                     |
| Med-PaLM       | 540B                                          | 2022    | PaLM        | ×           | Decoder-Only    | 780B     | HealthSearchQA, MedicationQA, LiveQA                                             |
| FLAN           | 137B                                          | 2021    | LaMDA-PT    | ✓           | Decoder-Only    | -        | Web documents, code, dialog data, Wikipedia                                      |
| Gopher         | 280B                                          | 2021    | -           | ×           | Decoder-Only    | 300B     | MassiveText                                                                      |
| BLOOM          | 176B                                          | 2022    | -           | ✓           | Decoder-Only    | 366B     | ROOTS                                                                            |
| Falcon 180B    | 180B                                          | 2023    | -           | ✓           | Decoder-Only    | 3.5T     | RefinedWeb                                                                       |
| TinyLlama-1.1B | 1.1B                                          | 2024    | LLaMA1.1B   | ✓           | Decoder-Only    | 3T       | SlimPajama, Starcoderdata                                                        |

#### Evalutaion metric

| Benchmark Name                | Evaluation Metric                                                                               | Leaderboard                                    | Source                                       | paper with code                                                      |
| ----------------------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------- | -------------------------------------------- | -------------------------------------------------------------------- |
| HumanEval                     | PASS@k                                                                                          | [Link](https://llm-leaderboard.streamlit.app/) | [Link](https://github.com/openai/human-eval) | [Link](https://paperswithcode.com/sota/code-generation-on-humaneval) |
| MBPP                          | PASS@k, Accuracy                                                                                | -                                              | [Link](#)                                    | [Link](#)                                                            |
| APPS                          | PASS@k, Accuracy                                                                                | -                                              | [Link](#)                                    | [Link](#)                                                            |
| WikiSQL                       | Accuracy                                                                                        | -                                              | [Link](#)                                    | [Link](#)                                                            |
| CoNaLa                        | BLEU                                                                                            | [Link](#)                                      | [Link](#)                                    | -                                                                    |
| CodeParrot                    | PASS@k                                                                                          | [Link](#)                                      | -                                            | -                                                                    |
| HellaSwag                     | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| AI2 Reasoning Challenge (ARC) | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| BoolQ                         | Accuracy                                                                                        | -                                              | [Link](#)                                    | [Link](#)                                                            |
| MultiRC                       | F1-score, Accuracy                                                                              | -                                              | [Link](#)                                    | [Link](#)                                                            |
| CNN/Daily Mail [200]          | Accuracy                                                                                        | -                                              | [Link](#)                                    | -                                                                    |
| SQuAD                         | F1-score, EM                                                                                    | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| RACE                          | Accuracy                                                                                        | -                                              | [Link](#)                                    | [Link](#)                                                            |
| CNN/Daily Mail [201]          | ROUGE                                                                                           | -                                              | [Link](#)                                    | [Link](#)                                                            |
| Drop                          | F1-score, EM                                                                                    | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| QuAC                          | F1-score, HEQ-Q, HEQ-D                                                                          | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| TriviaQA                      | EM, F1-score, Accuracy                                                                          | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| Natural Questions             | EM, F1-score, Accuracy                                                                          | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| StrategyQA                    | Accuracy, Recall@10, SARI                                                                       | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| CoQA                          | F1-score                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| XSum                          | ROUGE                                                                                           | -                                              | [Link](#)                                    | [Link](#)                                                            |
| SAMSum                        | ROUGE                                                                                           | -                                              | -                                            | [Link](#)                                                            |
| WikiSum                       | ROUGE                                                                                           | -                                              | [Link](#)                                    | -                                                                    |
| DialogSum                     | ROUGE                                                                                           | -                                              | [Link](#)                                    | [Link](#)                                                            |
| TruthfulQA                    | MC1, MC2, % true, % info, BLEURT                                                                | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| MMLU                          | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| GSM8K                         | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| PIQA                          | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| SIQA                          | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| OpenBookQA (OBQA)             | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| HotpotQA                      | EM, F1-score, Joint EM, Joint F1-score                                                          | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| MATH                          | Accuracy                                                                                        | -                                              | [Link](#)                                    | [Link](#)                                                            |
| CommonsenseQA                 | Accuracy                                                                                        | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| Natural Instructions          | ROUGE-L, Human                                                                                  | [Link](#)                                      | [Link](#)                                    | [Link](#)                                                            |
| BIG-bench                     | Accuracy, Average                                                                               | -                                              | [Link](#)                                    | [Link](#)                                                            |
| ToolTalk                      | Success rate, Precision, Recall, Incorrect action rate, Percent of failing error types          | -                                              | [Link](#)                                    | [Link](#)                                                            |
| MetaTool                      | Accuracy, Precision, Recall, F1-score                                                           | -                                              | [Link](#)                                    | [Link](#)                                                            |
| GPT4Tools                     | Success Rate of Thought, Action, Arguments, Overall Success                                     | -                                              | [Link](#)                                    | [Link](#)                                                            |
| API-Bank                      | Correctness, ROUGE, Error types (API Hallucination, Exceptions, Invalid Input Parameters, etc.) | -                                              | [Link](#)                                    | [Link](#)                                                            |
| Alpaca-CoT                    | -                                                                                               | -                                              | [Link](#)                                    | [Link](#)                                                            |
